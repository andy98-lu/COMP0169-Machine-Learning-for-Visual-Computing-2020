{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "import csv\n",
    "\n",
    "imgnet_labels = list(csv.reader(open('imagenet_classes.txt','r'),delimiter='\\n'))\n",
    "imgnet_syns = list(csv.reader(open('imagenet_synsets.txt','r'),delimiter=' '))\n",
    "synset_dict = {}\n",
    "for row in imgnet_syns:\n",
    "    synset_dict[row[0]] = row[1]\n",
    "label_dict = {}\n",
    "for i,label in enumerate(imgnet_labels):\n",
    "    label_name = synset_dict[label[0]]\n",
    "    label_dict[i] = label_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define your dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CelebA_Dataset(data.Dataset):\n",
    "    def __init__(self,path):\n",
    "        image_names = os.listdir(path)\n",
    "        self.image_paths = [os.path.join(path,name) for name in image_names]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.normalize(img, None, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the dataset instance and the torch dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "celeba_dataset = CelebA_Dataset(\"./random_celeba\")\n",
    "print(\"Number of images:\", len(celeba_dataset))\n",
    "print(celeba_dataset[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_celeba = DataLoader(celeba_dataset,batch_size=4,shuffle=True,num_workers=1)\n",
    "print(\"Number of batches:\",len(torch_celeba))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterate over batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index,data in enumerate(torch_celeba):\n",
    "    print(\"Batch number:\",index,\"\\t Shape of data:\",data.size())\n",
    "    # Optionally, if you have a GPU, you can move the data to it \n",
    "    #data = data.cuda() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# functions to show an image\n",
    "\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "class VGGClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGGClass, self).__init__()\n",
    "        blocks = []\n",
    "        self.vggnet = torchvision.models.vgg16(pretrained=True)\n",
    "        \n",
    "        self.transform = torch.nn.functional.interpolate\n",
    "        self.mean = torch.nn.Parameter(torch.tensor([0.485, 0.456, 0.406]).view(1,3,1,1))\n",
    "        self.std = torch.nn.Parameter(torch.tensor([0.229, 0.224, 0.225]).view(1,3,1,1))\n",
    "\n",
    "    def forward(self, input):\n",
    "        input = (input-self.mean) / self.std\n",
    "        input = self.transform(input, mode='bilinear', size=(224, 224), align_corners=False)\n",
    "        vggclass = self.vggnet(input)\n",
    "        return vggclass\n",
    "    \n",
    "vgg_class = VGGClass()\n",
    "\n",
    "for ind,data in enumerate(torch_celeba):\n",
    "    \n",
    "        \n",
    "    print(\"Data size:\", data.size())\n",
    "    \n",
    "    #PYTORCH EXPECTS DATA IN THE FOLLOWING SHAPE : [BATCH_SIZE, NUMBER OF CHANNELS, WIDTH,HEIGHT]\n",
    "    #OUR DATA LOADER IS RETURNING US THIS SHAPE : [BATCH_SIZE, WIDTH, HEIGHT, NUMBER_OF_CHANNELS]\n",
    "    \n",
    "    #SO WE CHANGE THE SHAPE NOW:\n",
    "    data = data.permute(0,3,1,2)\n",
    "    \n",
    "    #CHANGE DATA TYPE TO FLOAT, BECAUSE PYTORCH NETWORKS WANT FLOAT DATA\n",
    "    \n",
    "    data = data.float()\n",
    "    \n",
    "    \n",
    "    predicted_class = vgg_class(data)\n",
    "    print(\"Shape of Predicted classes\",predicted_class.size())\n",
    "    \n",
    "    \n",
    "    imshow(torchvision.utils.make_grid(data))\n",
    "\n",
    "    class_indices = torch.argmax(predicted_class,1)\n",
    "    \n",
    "    for i in class_indices:\n",
    "        print(label_dict[i.item()])\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
