{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "from autograd import jacobian,elementwise_grad,grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Simple Scalar Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixed, scalar inputs x,y and z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([5])\n",
    "y = np.array([3])\n",
    "z = np.array([2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The forward function, with 3 weights wx,wy,wz associated with each of x,y,z respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ScalarForward(wx,wy,wz):\n",
    "    p = wx*x + wy*y\n",
    "    q = wz*z\n",
    "    f = p * q\n",
    "    return f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### df/dwx = (df/dp)\\*(dp/dwx) = q*x\n",
    "### df/dwy = (df/dp)\\*(dp/dwy) = q*y\n",
    "### df/dwz = (df/dq)\\*(dq/dwz) = p*z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed Gradients: [[9.87425431]] [[5.92455258]] [[4.73717653]]\n"
     ]
    }
   ],
   "source": [
    "# pick weights randomly\n",
    "wx = np.random.random((1,1))\n",
    "wy = np.random.random((1,1))\n",
    "wz = np.random.random((1,1))\n",
    "\n",
    "# compute gradients of f w.r.t wx,wy,wz\n",
    "df_dwx = (wz*z)*x\n",
    "df_dwy = (wz*z)*y\n",
    "df_dwz = (wx*x + wy*y)*z\n",
    "\n",
    "print(\"Computed Gradients:\",df_dwx,df_dwy,df_dwz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify manually computed gradients with autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autograd Gradients: [[9.87425431]] [[5.92455258]] [[4.73717653]]\n"
     ]
    }
   ],
   "source": [
    "# autograd's `grad` returns a gradient function, that you can use later, with ScalarForward's arguments.\n",
    "# ScalarForward takes in wx,wy,wz at positions 0,1,2 in the argument list.\n",
    "# So, grad(ScalarForward,0) returns gradient of ScalarForward w.r.t wx.\n",
    "\n",
    "gradient_fn_wx = grad(ScalarForward,0) # function to compute gradient w.r.t wx\n",
    "gradient_fn_wy = grad(ScalarForward,1) # function to compute gradient w.r.t wy\n",
    "gradient_fn_wz = grad(ScalarForward,2) # function to compute gradient w.r.t wz\n",
    "\n",
    "#call gradient functions with the weights we picked randomly before.\n",
    "\n",
    "auto_df_dwx = gradient_fn_wx(wx,wy,wz)\n",
    "auto_df_dwy = gradient_fn_wy(wx,wy,wz)\n",
    "auto_df_dwz = gradient_fn_wz(wx,wy,wz)\n",
    "\n",
    "print(\"Autograd Gradients:\",auto_df_dwx,auto_df_dwy,auto_df_dwz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Network with vector input, no hidden layers\n",
    "\\begin{align}\n",
    "f = \\sigma(W^TX)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([  [0,0,1] ]) # Input vector\n",
    "Y = np.array([ [0] ]) # Target output\n",
    "W0 = np.random.random((3,1)) # random weight W0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activation function and its derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return sigmoid(x)*(1-sigmoid(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward Operations, step-by-step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "Linear = np.dot(X,W0) # Linear Operation\n",
    "NonLinear = sigmoid(Linear) # Apply Non-linear activation\n",
    "Error = Y - NonLinear # Compute Error w.r.t target Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backprop: Compute gradient of Error w.r.t W0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed Gradients:\n",
      "Gradient w.r.t W0: [[-0.        ]\n",
      " [-0.        ]\n",
      " [-0.22394959]]\n"
     ]
    }
   ],
   "source": [
    "# The gradient has to backprop from output, via the activation, to W0\n",
    "# Work through these steps, backwards. From the last equation in this cell, to the first.\n",
    "\n",
    "# Since Linear output = W^T*X\n",
    "dLinear_W0 = X.T\n",
    "\n",
    "# For NonLinear w.r.t W0, we need NonLinear w.r.t Linear via sigmoid , then Linear w.r.t W0\n",
    "dNonLinear_W0 = dLinear_W0 * sigmoid_derivative(Linear)\n",
    "\n",
    "# Second Step: Trivial\n",
    "dError_NonLinear = -1\n",
    "\n",
    "# First step: For gradient of Error w.r.t W0, first find its gradient w.r.t to NonLinear.\n",
    "#             Then find gradient of NonLinear w.r.t W0\n",
    "dError_W0 = dNonLinear_W0 * dError_NonLinear\n",
    "\n",
    "print(\"Computed Gradients:\")\n",
    "print(\"Gradient w.r.t W0:\", dError_W0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backprop: Verification by autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autograd Gradients:\n",
      "Gradient w.r.t W0: [[ 0.        ]\n",
      " [ 0.        ]\n",
      " [-0.22394959]]\n"
     ]
    }
   ],
   "source": [
    "def VectorForward(W0):\n",
    "    Linear = np.dot(X,W0)\n",
    "    NonLinear = sigmoid(Linear)\n",
    "    Error = Y - NonLinear\n",
    "    return Error \n",
    "\n",
    "gradient_fn_W0 = grad(VectorForward)\n",
    "\n",
    "autograd_dError_W0 = gradient_fn_W0(W0)\n",
    "print(\"Autograd Gradients:\")\n",
    "print(\"Gradient w.r.t W0:\", autograd_dError_W0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Vector Input, One Hidden Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input X, Target Y , and 2 Weights because we have 1 hidden layer.\n",
    "X = np.array([  [0,0,1] ])\n",
    "Y = np.array([ [0] ])\n",
    "W0 = np.random.random((3,4))\n",
    "W1 = np.random.random((4,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward Operations, step-by-step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "L1_Input = np.dot(X,W0) # Unactivated input to the hidden layer\n",
    "L1 = sigmoid(L1_Input) # Hidden Layer activated output\n",
    "\n",
    "Last_Input = np.dot(L1,W1)\n",
    "Pred = sigmoid(Last_Input)\n",
    "\n",
    "Error = Y - Pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed Gradients:\n",
      "Gradient w.r.t W1: [[-0.11468638]\n",
      " [-0.15016915]\n",
      " [-0.13416744]\n",
      " [-0.12815256]]\n",
      "Gradient w.r.t W0: [[-0.         -0.         -0.         -0.        ]\n",
      " [-0.         -0.         -0.         -0.        ]\n",
      " [-0.02617879 -0.00168374 -0.03976565 -0.0017205 ]]\n"
     ]
    }
   ],
   "source": [
    "# Compute the gradient w.r.t last weight before output.\n",
    "dLast_Input_W1 = L1.T\n",
    "dPred_W1 = dLast_Input_W1 * sigmoid_derivative(Last_Input)\n",
    "dError_Pred = -1\n",
    "dError_W1 = dPred_W1 * dError_Pred   # This is gradient of Error w.r.t. W1\n",
    "\n",
    "# Now compute the Upstream gradient - i.e, compute gradient of Error w.r.t the output of the previous layer.\n",
    "dLast_Input_L1 = W1.T\n",
    "dPred_L1 =  dLast_Input_L1 * sigmoid_derivative(Last_Input)\n",
    "dError_L1 = dPred_L1 * dError_Pred  # Gradient of Error w.r.t Layer output\n",
    "\n",
    "# Now use Upstream Gradient and comput gradient of Error w.r.t the weights in the layer before that.\n",
    "dL1_Input_W0 = X.T\n",
    "dL1_W0 = dL1_Input_W0 * sigmoid_derivative(L1_Input)\n",
    "dError_W0 = dL1_W0 * dError_L1 # Gradient of Error w.r.t W0\n",
    "\n",
    "print(\"Computed Gradients:\")\n",
    "print(\"Gradient w.r.t W1:\", dError_W1)\n",
    "print(\"Gradient w.r.t W0:\", dError_W0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verify computed gradients using autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autograd Gradients:\n",
      "Gradient w.r.t W1: [[-0.11468638]\n",
      " [-0.15016915]\n",
      " [-0.13416744]\n",
      " [-0.12815256]]\n",
      "Gradient w.r.t W0: [[ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [-0.02617879 -0.00168374 -0.03976565 -0.0017205 ]]\n"
     ]
    }
   ],
   "source": [
    "def vector_one_hidden_forward(W0,W1):\n",
    "    L1_Input = np.dot(X,W0)\n",
    "    L1 = sigmoid(L1_Input)\n",
    "\n",
    "    Last_Input = np.dot(L1,W1)\n",
    "    Pred = sigmoid(Last_Input)\n",
    "\n",
    "    Error = Y - Pred\n",
    "\n",
    "    return Error\n",
    "\n",
    "autogradient_fn_W0 = grad(vector_one_hidden_forward,0)\n",
    "autogradient_fn_W1 = grad(vector_one_hidden_forward,1)\n",
    "\n",
    "autograd_dError_W0 = autogradient_fn_W0(W0,W1)\n",
    "autograd_dError_W1 = autogradient_fn_W1(W0,W1)\n",
    "\n",
    "print(\"Autograd Gradients:\")\n",
    "print(\"Gradient w.r.t W1:\", autograd_dError_W1)\n",
    "print(\"Gradient w.r.t W0:\", autograd_dError_W0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Two Hidden Layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 Hidden Layers, so 3 Weights now to compute for\n",
    "X = np.array([  [1,0,1] ])\n",
    "Y = np.array([ [0] ])\n",
    "W0 = np.random.random((3,4))\n",
    "W1 = np.random.random((4,4))\n",
    "W2 = np.random.random((4,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "L1_Input = np.dot(X,W0)\n",
    "L1 = sigmoid(L1_Input)\n",
    "\n",
    "L2_Input = np.dot(L1,W1)\n",
    "L2 = sigmoid(L2_Input)\n",
    "\n",
    "Last_Input = np.dot(L2,W2)\n",
    "Pred = sigmoid(Last_Input)\n",
    "\n",
    "Error = Y - Pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill this section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autograd Gradients:\n",
      "Gradient w.r.t W2: [[-0.05355395]\n",
      " [-0.06044564]\n",
      " [-0.06279225]\n",
      " [-0.06094853]]\n",
      "Gradient w.r.t W1: [[-0.00800912 -0.00398036 -0.00437293 -0.00224655]\n",
      " [-0.00826884 -0.00410943 -0.00451473 -0.0023194 ]\n",
      " [-0.00838213 -0.00416574 -0.00457659 -0.00235118]\n",
      " [-0.00836132 -0.0041554  -0.00456523 -0.00234535]]\n",
      "Gradient w.r.t W0: [[-0.00286654 -0.00353481 -0.00071599 -0.00270369]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [-0.00286654 -0.00353481 -0.00071599 -0.00270369]]\n"
     ]
    }
   ],
   "source": [
    "def vector_two_hidden_forward(W0,W1,W2):\n",
    "    L1_Input = np.dot(X,W0)\n",
    "    L1 = sigmoid(L1_Input)\n",
    "    L2_Input = np.dot(L1,W1)\n",
    "    L2 = sigmoid(L2_Input)\n",
    "    Last_Input = np.dot(L2,W2)\n",
    "    Pred = sigmoid(Last_Input)\n",
    "    Error = Y - Pred\n",
    "    \n",
    "    return Error\n",
    "\n",
    "autogradient_fn_W0 = grad(vector_two_hidden_forward,0)\n",
    "autogradient_fn_W1 = grad(vector_two_hidden_forward,1)\n",
    "autogradient_fn_W2 = grad(vector_two_hidden_forward,2)\n",
    "\n",
    "autograd_dError_W0 = autogradient_fn_W0(W0,W1,W2)\n",
    "autograd_dError_W1 = autogradient_fn_W1(W0,W1,W2)\n",
    "autograd_dError_W2 = autogradient_fn_W2(W0,W1,W2)\n",
    "\n",
    "print(\"Autograd Gradients:\")\n",
    "print(\"Gradient w.r.t W2:\", autograd_dError_W2)\n",
    "print(\"Gradient w.r.t W1:\", autograd_dError_W1)\n",
    "print(\"Gradient w.r.t W0:\", autograd_dError_W0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
